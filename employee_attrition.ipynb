{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9086f5d0-55da-451c-b412-9e32211c4618",
   "metadata": {},
   "source": [
    "# Employee Attrition Prediction Using Ensemble Methods (XGBoost + LightGBM)\n",
    "\n",
    "This notebook demonstrates how to predict employee attrition using an ensemble of XGBoost and LightGBM models. The dataset is highly imbalanced, so we will use techniques like SMOTE-Tomek and ADASYN to handle class imbalance. We will also perform hyperparameter tuning using Optuna to optimize the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac3a54-9053-44d6-acd0-cb2a633e5f79",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "First, we import all the necessary libraries for data processing, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46721715-a37b-4559-b3f4-0103b2496690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1deba-9aac-4ce5-9d28-c3b0b8e1e1ee",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset\n",
    "We load the dataset using **kagglehub**. Replace the dataset path with your actual dataset path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e983b8ad-96a6-4415-a917-209dc28b52a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee_ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Department</th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Job_Level</th>\n",
       "      <th>Monthly_Income</th>\n",
       "      <th>Hourly_Rate</th>\n",
       "      <th>Years_at_Company</th>\n",
       "      <th>...</th>\n",
       "      <th>Overtime</th>\n",
       "      <th>Project_Count</th>\n",
       "      <th>Average_Hours_Worked_Per_Week</th>\n",
       "      <th>Absenteeism</th>\n",
       "      <th>Work_Environment_Satisfaction</th>\n",
       "      <th>Relationship_with_Manager</th>\n",
       "      <th>Job_Involvement</th>\n",
       "      <th>Distance_From_Home</th>\n",
       "      <th>Number_of_Companies_Worked</th>\n",
       "      <th>Attrition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>Female</td>\n",
       "      <td>Married</td>\n",
       "      <td>IT</td>\n",
       "      <td>Manager</td>\n",
       "      <td>1</td>\n",
       "      <td>15488</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>Female</td>\n",
       "      <td>Married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Assistant</td>\n",
       "      <td>5</td>\n",
       "      <td>13079</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>Male</td>\n",
       "      <td>Married</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Assistant</td>\n",
       "      <td>1</td>\n",
       "      <td>13744</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>Female</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Manager</td>\n",
       "      <td>1</td>\n",
       "      <td>6809</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>Male</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Executive</td>\n",
       "      <td>1</td>\n",
       "      <td>10206</td>\n",
       "      <td>52</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Employee_ID  Age  Gender Marital_Status Department   Job_Role  Job_Level  \\\n",
       "0            1   58  Female        Married         IT    Manager          1   \n",
       "1            2   48  Female        Married      Sales  Assistant          5   \n",
       "2            3   34    Male        Married  Marketing  Assistant          1   \n",
       "3            4   27  Female       Divorced  Marketing    Manager          1   \n",
       "4            5   40    Male       Divorced  Marketing  Executive          1   \n",
       "\n",
       "   Monthly_Income  Hourly_Rate  Years_at_Company  ...  Overtime  \\\n",
       "0           15488           28                15  ...        No   \n",
       "1           13079           28                 6  ...       Yes   \n",
       "2           13744           24                24  ...       Yes   \n",
       "3            6809           26                10  ...        No   \n",
       "4           10206           52                29  ...        No   \n",
       "\n",
       "   Project_Count  Average_Hours_Worked_Per_Week  Absenteeism  \\\n",
       "0              6                             54           17   \n",
       "1              2                             45            1   \n",
       "2              6                             34            2   \n",
       "3              9                             48           18   \n",
       "4              3                             33            0   \n",
       "\n",
       "   Work_Environment_Satisfaction  Relationship_with_Manager Job_Involvement  \\\n",
       "0                              4                          4               4   \n",
       "1                              4                          1               2   \n",
       "2                              3                          4               4   \n",
       "3                              2                          3               1   \n",
       "4                              4                          1               3   \n",
       "\n",
       "   Distance_From_Home  Number_of_Companies_Worked  Attrition  \n",
       "0                  20                           3         No  \n",
       "1                  25                           2         No  \n",
       "2                  45                           3         No  \n",
       "3                  35                           3         No  \n",
       "4                  44                           3         No  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data using kagglehub\n",
    "path = kagglehub.dataset_download(\"ziya07/employee-attrition-prediction-dataset\")\n",
    "df = pd.read_csv(f\"{path}/employee_attrition_dataset.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319db3a8-c7f4-452c-8a9e-2e0457e1a1ef",
   "metadata": {},
   "source": [
    "## 3. Prepare the Data\n",
    "We preprocess the dataset by dropping irrelevant columns, performing feature engineering, and splitting the data into features (X) and target (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a9b76f-25c2-4e63-b8c2-a411a4111e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1000, 28)\n",
      "Target shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(df):\n",
    "    \"\"\"Prepare the dataset with advanced feature engineering.\"\"\"\n",
    "    # Drop irrelevant columns\n",
    "    df = df.drop(columns=['Employee_ID'])\n",
    "    \n",
    "    # Feature engineering\n",
    "    df['Salary_to_Experience'] = df['Monthly_Income'] / (df['Years_at_Company'] + 1)\n",
    "    df['Overtime_Impact'] = df['Overtime'].map({'Yes': 1, 'No': 0}) * df['Monthly_Income']\n",
    "    df['Years_At_Company_Ratio'] = df['Years_at_Company'] / (df['Number_of_Companies_Worked'] + 1)\n",
    "    df['Promotion_Rate'] = df['Years_at_Company'] / (df['Years_Since_Last_Promotion'] + 1)\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df.drop(columns=['Attrition'])\n",
    "    y = df['Attrition'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Prepare the data\n",
    "X, y = prepare_data(df)\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179d31a-4d8d-4aef-afe5-cd800ce74702",
   "metadata": {},
   "source": [
    "## 4. Create a Preprocessor\n",
    "We create a preprocessor to handle numerical and categorical features separately. Numerical features are scaled, and categorical features are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ac980a-586b-4179-b6ef-1eff0983e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(X):\n",
    "    \"\"\"Create a preprocessor with separate handling for numerical and categorical features.\"\"\"\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    numerical_cols = X.select_dtypes(exclude=['object']).columns\n",
    "    \n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = create_preprocessor(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65fa82-a38f-48c5-9e2f-b6755f285817",
   "metadata": {},
   "source": [
    "## 5. Define the Optuna Objective Function\n",
    "We define an Optuna objective function to optimize the hyperparameters of XGBoost and LightGBM. The objective function uses cross-validation to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8006f4d7-4878-4020-a808-9e003c33d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y, preprocessor):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
    "    # LightGBM parameters\n",
    "    lgb_params = {\n",
    "        'n_estimators': trial.suggest_int('lgb_n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('lgb_learning_rate', 1e-3, 0.1),\n",
    "        'num_leaves': trial.suggest_int('lgb_num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('lgb_max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('lgb_min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_uniform('lgb_subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('lgb_colsample_bytree', 0.6, 1.0),\n",
    "        'class_weight': 'balanced',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('xgb_n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
    "        'max_depth': trial.suggest_int('xgb_max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('xgb_min_child_weight', 1, 7),\n",
    "        'subsample': trial.suggest_uniform('xgb_subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('xgb_colsample_bytree', 0.6, 1.0),\n",
    "        'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 1, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Create models\n",
    "    lgb = LGBMClassifier(**lgb_params)\n",
    "    xgb = XGBClassifier(**xgb_params)\n",
    "    \n",
    "    # Create ensemble\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lgb', lgb),\n",
    "            ('xgb', xgb)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    # Create pipeline with SMOTE-Tomek and ADASYN\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('tomek', TomekLinks()),\n",
    "        ('adasyn', ADASYN(random_state=42)),\n",
    "        ('classifier', ensemble)\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Use PR-AUC as the metric for imbalanced classification\n",
    "        precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        scores.append(pr_auc)\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f9d01-cc5c-48c8-ae44-8afcda65add6",
   "metadata": {},
   "source": [
    "## 6. Train and Evaluate the Model\n",
    "We split the data into training and testing sets, optimize the hyperparameters using Optuna, and train the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a147ea7-751b-43d8-ae7a-80493b201abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 18:27:16,109] A new study created in memory with name: no-name-0fae114d-63f0-46a4-9403-7b5250ac4bd0\n",
      "[I 2025-01-29 18:27:21,835] Trial 0 finished with value: 0.19499344415236824 and parameters: {'lgb_n_estimators': 994, 'lgb_learning_rate': 0.0010134261690233537, 'lgb_num_leaves': 75, 'lgb_max_depth': 5, 'lgb_min_child_samples': 82, 'lgb_subsample': 0.6102034500232334, 'lgb_colsample_bytree': 0.9068875517204755, 'xgb_n_estimators': 536, 'xgb_learning_rate': 0.019028931063476143, 'xgb_max_depth': 6, 'xgb_min_child_weight': 3, 'xgb_subsample': 0.8647885125787671, 'xgb_colsample_bytree': 0.6283259480693634, 'scale_pos_weight': 4.525858736464565}. Best is trial 0 with value: 0.19499344415236824.\n",
      "[I 2025-01-29 18:27:25,529] Trial 1 finished with value: 0.1922710128131687 and parameters: {'lgb_n_estimators': 246, 'lgb_learning_rate': 0.005539061102584841, 'lgb_num_leaves': 97, 'lgb_max_depth': 5, 'lgb_min_child_samples': 43, 'lgb_subsample': 0.9593342152953701, 'lgb_colsample_bytree': 0.6974498291540706, 'xgb_n_estimators': 666, 'xgb_learning_rate': 0.06529292211512197, 'xgb_max_depth': 7, 'xgb_min_child_weight': 6, 'xgb_subsample': 0.9574542035184146, 'xgb_colsample_bytree': 0.7119878052938726, 'scale_pos_weight': 2.227196071720666}. Best is trial 0 with value: 0.19499344415236824.\n",
      "[I 2025-01-29 18:27:37,573] Trial 2 finished with value: 0.1940496310295075 and parameters: {'lgb_n_estimators': 586, 'lgb_learning_rate': 0.0025773356213979644, 'lgb_num_leaves': 42, 'lgb_max_depth': 5, 'lgb_min_child_samples': 31, 'lgb_subsample': 0.9654344105577233, 'lgb_colsample_bytree': 0.9389693824147455, 'xgb_n_estimators': 735, 'xgb_learning_rate': 0.005130538339141772, 'xgb_max_depth': 10, 'xgb_min_child_weight': 7, 'xgb_subsample': 0.6858625880438057, 'xgb_colsample_bytree': 0.6832291543398494, 'scale_pos_weight': 7.783899440458686}. Best is trial 0 with value: 0.19499344415236824.\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(X, y):\n",
    "    \"\"\"Train the final model and evaluate its performance.\"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = create_preprocessor(X)\n",
    "    \n",
    "    # Optimize hyperparameters\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, preprocessor), n_trials=20)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Create final model with best parameters\n",
    "    lgb = LGBMClassifier(\n",
    "        verbosity=-1,\n",
    "        n_estimators=best_params['lgb_n_estimators'],\n",
    "        learning_rate=best_params['lgb_learning_rate'],\n",
    "        num_leaves=best_params['lgb_num_leaves'],\n",
    "        max_depth=best_params['lgb_max_depth'],\n",
    "        min_child_samples=best_params['lgb_min_child_samples'],\n",
    "        subsample=best_params['lgb_subsample'],\n",
    "        colsample_bytree=best_params['lgb_colsample_bytree'],\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=best_params['xgb_n_estimators'],\n",
    "        learning_rate=best_params['xgb_learning_rate'],\n",
    "        max_depth=best_params['xgb_max_depth'],\n",
    "        min_child_weight=best_params['xgb_min_child_weight'],\n",
    "        subsample=best_params['xgb_subsample'],\n",
    "        colsample_bytree=best_params['xgb_colsample_bytree'],\n",
    "        scale_pos_weight=best_params['scale_pos_weight'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lgb', lgb),\n",
    "            ('xgb', xgb)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    # Create final pipeline\n",
    "    final_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('tomek', TomekLinks()),\n",
    "        ('adasyn', ADASYN(random_state=42)),\n",
    "        ('classifier', ensemble)\n",
    "    ])\n",
    "    \n",
    "    # Train final model\n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = final_pipeline.predict(X_test)\n",
    "    y_pred_proba = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
    "    print(f\"PR-AUC: {pr_auc:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return final_pipeline\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = train_and_evaluate_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2a685-8661-4c9d-96b0-f9717d7349e1",
   "metadata": {},
   "source": [
    "## 7. Save the Model\n",
    "Finally, we save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49ebed78-5ca1-4c2d-ae2b-0e622e92f9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attrition_ensemble_model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'attrition_ensemble_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c239e76-3183-4330-b0b6-142e407dc509",
   "metadata": {},
   "source": [
    "# Dataset Summary\n",
    "### 1. Dataset Overview\n",
    "**Source:** The dataset is likely sourced from HR records or employee surveys, containing information about employees and whether they left the company (attrition).\n",
    "\n",
    "**Target Variable:** Attrition (binary: \"Yes\" or \"No\").\n",
    "\n",
    "**Features:** The dataset includes features such as:\n",
    "\n",
    "Demographic information (e.g., Age, Gender, Marital_Status).\n",
    "\n",
    "Job-related information (e.g., Department, Job_Role, Job_Level).\n",
    "\n",
    "Financial information (e.g., Monthly_Income, Hourly_Rate).\n",
    "\n",
    "Work history (e.g., Years_at_Company, Years_in_Current_Role).\n",
    "\n",
    "Satisfaction metrics (e.g., Job_Satisfaction, Work_Life_Balance).\n",
    "\n",
    "Behavioral metrics (e.g., Overtime, Absenteeism).\n",
    "\n",
    "### 2. Class Imbalance\n",
    "Imbalance Ratio: The dataset is highly imbalanced, with the majority of employees belonging to the \"No Attrition\" class and a small percentage belonging to the \"Attrition\" class.\n",
    "\n",
    "For example, if 95% of the data is \"No Attrition\" and 5% is \"Attrition,\" the model will naturally favor the majority class.\n",
    "\n",
    "Impact on Model Performance:\n",
    "\n",
    "Models trained on imbalanced datasets tend to have poor predictive power for the minority class (attrition).\n",
    "\n",
    "Metrics like accuracy can be misleading, as the model may achieve high accuracy by simply predicting the majority class.\n",
    "\n",
    "### 3. Challenges in Building a Strong Predictive Model\n",
    "#### a. Limited Information for the Minority Class\n",
    "The small number of attrition cases makes it difficult for the model to learn meaningful patterns for predicting attrition.\n",
    "\n",
    "Techniques like oversampling (e.g., SMOTE, ADASYN) can help, but they may introduce noise or overfitting.\n",
    "\n",
    "#### b. Lack of Predictive Features\n",
    "The dataset may not contain features that strongly correlate with attrition. For example:\n",
    "\n",
    "Factors like employee morale, workplace culture, or personal circumstances are often not captured in HR datasets.\n",
    "\n",
    "Without these features, the model may struggle to identify the root causes of attrition.\n",
    "\n",
    "#### c. Noisy or Irrelevant Features\n",
    "Some features in the dataset may be irrelevant or noisy, reducing the model's predictive power.\n",
    "\n",
    "For example, features like Employee_ID or Hourly_Rate may not provide useful information for predicting attrition.\n",
    "\n",
    "#### d. Data Quality Issues\n",
    "Missing values, outliers, or inconsistencies in the dataset can negatively impact model performance.\n",
    "\n",
    "For example, if Monthly_Income has missing values or outliers, it may skew the model's predictions.\n",
    "\n",
    "#### e. Domain-Specific Complexity\n",
    "Attrition is influenced by complex, often subjective factors (e.g., employee satisfaction, work-life balance) that are difficult to quantify.\n",
    "\n",
    "Without domain-specific features or external data (e.g., employee surveys), the model may lack the necessary information to make accurate predictions.\n",
    "\n",
    "### 4. Recommendations for Improving the Dataset\n",
    "**a.** Collect More Data\n",
    "Gather additional data, especially for the minority class (attrition cases).\n",
    "\n",
    "Use external data sources (e.g., employee surveys, industry turnover rates) to enrich the dataset.\n",
    "\n",
    "**b.** Feature Engineering\n",
    "Create new features that better capture the reasons for attrition. For example:\n",
    "\n",
    "Workload_Score: A metric combining Project_Count and Average_Hours_Worked_Per_Week.\n",
    "\n",
    "Satisfaction_Index: A composite score based on Job_Satisfaction, Work_Life_Balance, and Work_Environment_Satisfaction.\n",
    "\n",
    "**c.** Address Class Imbalance\n",
    "Use advanced resampling techniques like SMOTE, ADASYN, or class-weight adjustments to give more importance to the minority class.\n",
    "\n",
    "Consider anomaly detection or one-class classification techniques if the imbalance is extreme.\n",
    "\n",
    "**d.** Improve Data Quality\n",
    "Handle missing values, remove outliers, and correct inconsistencies in the dataset.\n",
    "\n",
    "Use robust preprocessing techniques to ensure the data is in a usable format.\n",
    "\n",
    "**e.** Incorporate Domain Knowledge\n",
    "Consult HR or domain experts to identify key drivers of attrition and ensure they are adequately represented in the dataset.\n",
    "\n",
    "Use domain-specific insights to guide feature engineering and model selection.\n",
    "\n",
    "### 5. Conclusion\n",
    "The dataset's class imbalance and potential lack of predictive features make it challenging to build a strong predictive model for employee attrition. While techniques like resampling and feature engineering can help, the dataset itself may need to be improved by collecting more data, enhancing feature quality, and incorporating domain-specific insights. Addressing these challenges is critical for building a model with strong predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
